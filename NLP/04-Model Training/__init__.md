**Generalization** in machine learning refers to a model's ability to perform well on new, unseen data that was not part of the training dataset. It is a crucial property indicating that the model has learned the underlying patterns and relationships within the training data, rather than merely memorizing the training examples.

**Grokking** refers to the surprising phenomenon of delayed generalization where neural networks, generalize on certain learning problems, long after overfitting their training set. 
![[Pasted image 20240609134918.png]]
Since Grokking requires extended model training, it runs contrary to the traditional Machine Learning belief that we need to stop model training early to prevent overfitting.
> [!info] Grokking references:
> https://www.youtube.com/watch?v=QgOeWbW0jeA

Grokking 