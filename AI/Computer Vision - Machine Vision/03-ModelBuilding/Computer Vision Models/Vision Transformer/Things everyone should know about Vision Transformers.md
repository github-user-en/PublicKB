> [!info] Reference:
>- [Three things everyone should know about Vision Transformers](https://github.com/github-user-en/PublicKB/blob/9fbd47011ecb457d813efaf59b8488c7ee5c4ef3/AI/Computer%20Vision%20-%20Machine%20Vision/03-ModelBuilding/Computer%20Vision%20Models/Vision%20Transformer/Three%20things%20everyone%20should%20know%20about%20Vision%20Transformers.pdf)

1. **Parallel vision transformers:**
	1. The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy.
	2. One can parallelize the architecture by reorganizing the Multi-Head Self Attention (MHSA) and the Feed-Forward Network (FFN) blocks by pairs, which can be done for any different numbers of parallel blocks. This produces an architecture with the same number of parameters and compute, while being wider and shallower. This design allows for more parallel processing, easing optimization and reducing latency depending on the implementation. 
	   ![[Pasted image 20240702171322.png]]
2. Typically one would train a model at a lower resolution than the one employed at inference time. This not only saves resources, but also reduces the discrepancy of scale between train and test images that results from data augmentation
3. **Fine-tuning attention is all you need:**
	- Fine-tuning the weights of the attention layers, while keeping the feedforward network (FFN) layers frozen, is usually sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks.
4. **Patch preprocessing with masked self-supervised learning:**
	- The first layers of a transformer have a relatively local span, suggesting that they mostly behave like convolutions. Some recent hybrid architectures preprocess their input images with a convolutional stem, to improve accuracy and training stability. However, preprocessing images with convolutions is a priori not compatible with the recent and successful mask-based self-supervised learning approaches, like BeiT or MAE. The convolutions propagate information across patches, impeding the masked prediction task.
	- To address this issue, we introduce a hierarchical MLP (hMLP) stem that interleaves MLP layers and patch aggregation operations, and prohibits any communication between patches. Our experiments show that this choice is effective and able to leverage the benefit of both BeiT self-supervised pre-training and patch pre-processing. Moreover, our hMLP-stem is also effective for ViT in the supervised case: it is on par with the best convolutional stem of our comparison
	- Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking.