# Dall-E
Dall-E works in a 3-step process:
1. It encodes the user's text prompt;
2. It looks through its latent space to find an image's embedding similar to the user prompt embedding (similarity measured using Cosine Similarity, I guess);
3. Considering the found image embedding as the Prior, it generates probable images for the user.

# CLIP
> [!info] Resources
> - https://www.youtube.com/watch?v=8sVgLz3-W_s
> - https://openai.com/index/clip/
> - https://www.youtube.com/watch?v=jwZQD0Cqz4o

CLIP is part of a group of papers (or, comity of models) revisiting learning visual representations from natural language supervision. Just that it uses more modern architectures (like the Transformer architecture) and includes VirTex,[33](https://openai.com/index/clip/#citation-bottom-33) which explored autoregressive language modeling, ICMLM,[34](https://openai.com/index/clip/#citation-bottom-34) which investigated masked language modeling, and ConVIRT,[35](https://openai.com/index/clip/#citation-bottom-35) which studied the same contrastive objective we use for CLIP but in the field of medical imaging.
- For this zero-shot classification task, OpenAI originally explored training image-to-caption language models (an image-to-text approach, similar to VirTex), but found this approach struggled at zero-shot transfer across multiple benchmarks.

The CLIP model contains two encoders: a Text encoder and an Image encoder. Whichever form of input it's given, it feeds it to the appropriate encoder.
CLIP is a model use by OpenAI to train Dall-E. CLIP generates 
1. CLIP generates the embeddings of the user's text prompt.
2. The images generated by Dall-E are also fed to CLIP to generate embedding for the image. 
3. The two embeddings are compared to determine whether they are in sync.
As such, CLIP was trained to learn & represent visual concepts from natural language supervision (i.e., the associated text caption/description). CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized.

## CLIP Achievements
1. It demonstrated that scaling a simple pre-training task is sufficient to achieve competitive zero-shot performance on a great variety of image classification datasets. CLIP models can be applied to nearly arbitrary visual classification tasks.
	- The pre-training task it scales is that of image-caption association. Previously, this contrastive approach was adopted by ConVIRT, but was limited to the field of medical imaging.
2. Leveraging contrastive learning, the model learn 1 embedding representation for an object's  textual and visual representation. 
3. **Better generalizability of the zero-shot classifier:** The aforementioned approach of learning  visual concepts directly from natural language helps CLIP generalize well over several benchmarks, even though measure different vision aspects. For instance,
		1. ObjectNet checks a model’s ability to recognize objects in many different poses and with many different backgrounds inside homes; while 
		2. ImageNet Rendition and ImageNet Sketch check a model’s ability to recognize more abstract depictions of objects.
> [!info]
> OpenAI conjectured that the previous classifiers suffered from poor generalizability because the models “cheat” by only optimizing for performance on the benchmark, much like a student who passed an exam by studying only the questions on past years’ exams.
> To verify the “cheating hypothesis”, we also measure how CLIP’s performance changes when it is able to “study” for ImageNet. When a linear classifier is fitted on top of CLIP’s features, it improves CLIP’s accuracy on the ImageNet test set by almost 10%. However, this classifier does _no better_ on average across an evaluation suite of 7 other datasets measuring “robust” performance.[30](https://openai.com/index/clip/#citation-bottom-30)

> [!info]
> To validate that the CLIP models are more flexible and general than the existing ImageNet models in their zero-shot performance over many different tasks, OpenAI measured CLIP’s zero-shot performance on over 30 different datasets including tasks such as fine-grained object classification, geo-localization, action recognition in videos, and OCR.[B](https://openai.com/index/clip/#citation-bottom-B) In particular, learning OCR was an example of an exciting behavior that did not occur in standard ImageNet models.

4. OpenAI demonstrated that creating dataset for such a capable zero-shot classifier could be cheaper than previous efforts. They used an abundantly available source of supervision: the text paired with images found across the internet. As such, they didn't need human labelers to create this dataset, and dataset preparation proved to be cheap. 
	- In comparison, the ImageNet dataset, one of the largest efforts in this space, required over 25,000 workers to annotate 14 million images for 22,000 object categories.
5. **CLIP is highly efficient:** CLIP learns from unfiltered, highly varied, and highly noisy data, and is intended to be used in a zero-shot manner. OpenAI knew from GPT-2 and 3 that models trained on such data can achieve compelling zero shot performance; however, such models require significant training compute. 
6. Despite their noisy dataset, training over which would have required significant training compute, two algorithmic choices led to significant compute savings:
	1. For this zero-shot classification task, OpenAI originally explored training image-to-caption language models (an image-to-text approach, similar to VirTex), but found this approach struggled at zero-shot transfer across multiple benchmarks. In small to medium scale experiments, we found that the contrastive objective used by CLIP is 4x to 10x more efficient at zero-shot ImageNet classification.
		- In the initial 16 GPU day experiment, using the VirTex approach, OpenAI observed that a language model achieved only 16% accuracy on ImageNet after training for 400 million images. In contrast, CLIP is much more efficient and achieved the same accuracy roughly 10x faster.
	2. The second choice was the adoption of the Vision Transformer,[36](https://openai.com/index/clip/#citation-bottom-36) which gave us a further 3x gain in compute efficiency over a standard ResNet. In the end, our best performing CLIP model trains on 256 GPUs for 2 weeks which is similar to existing large scale image models.[37](https://openai.com/index/clip/#citation-bottom-37), [23](https://openai.com/index/clip/#citation-bottom-23), [38](https://openai.com/index/clip/#citation-bottom-38), [36](https://openai.com/index/clip/#citation-bottom-36)


## CLIP Training approach
1. OpenAI scraped the internet for images-text pairs.
2. From this dataset, they would sample 1 image and 32,768 text snippets.
	- All of the classes in the dataset are converted into captions such as “a photo of a dog”.
3. CLIP was then given this proxy training task: given an image, predict which out of a set of 32,768 randomly sampled text snippets, was actually paired with it in our dataset.
	- The intuition behind this training task was that the CLIP models will need to learn to recognize a wide variety of visual concepts in images and associate them with their names.
	- 
> [!info] 
> OpenAI's article talks of CLIP in plural, as *"CLIP models"* because CLIP is composed of two encoder models: one image encoder model; other text encoder model.


## CLIP - Limitations
1. While CLIP usually performs well on recognizing common objects, it struggles on: 
	1. more abstract or systematic tasks such as counting the number of objects in an image; and on 
	2. more complex tasks such as predicting how close the nearest car is in a photo. 
	On these two datasets, zero-shot CLIP is only slightly better than random guessing. 
2. Zero-shot CLIP also struggles compared to task specific models on very fine-grained classification, such as telling the difference between car models, variants of aircraft, or flower species.
3. CLIP also still has poor generalization to images not covered in its pre-training dataset. For instance, although CLIP learns a capable OCR system, when evaluated on handwritten digits from the MNIST dataset, zero-shot CLIP only achieves 88% accuracy, well below the 99.75% of humans on the dataset. 
4. Finally, we’ve observed that CLIP’s zero-shot classifiers can be sensitive to wording or phrasing and sometimes require trial and error “prompt engineering” to perform well.


