# Dall-E
Dall-E works in a 3-step process:
1. It encodes the user's text prompt;
2. It looks through its latent space to find an image's embedding similar to the user prompt embedding (similarity measured using Cosine Similarity, I guess);
3. Considering the found image embedding as the Prior, it generates probable images for the user.

# CLIP
> [!info] Resources
> - https://www.youtube.com/watch?v=8sVgLz3-W_s
> - https://openai.com/index/clip/
> - https://www.youtube.com/watch?v=jwZQD0Cqz4o

CLIP is part of a group of papers (or, comity of models) revisiting learning visual representations from natural language supervision. Just that it uses more modern architectures (like the Transformer architecture) and includes VirTex,[33](https://openai.com/index/clip/#citation-bottom-33) which explored autoregressive language modeling, ICMLM,[34](https://openai.com/index/clip/#citation-bottom-34) which investigated masked language modeling, and ConVIRT,[35](https://openai.com/index/clip/#citation-bottom-35) which studied the same contrastive objective we use for CLIP but in the field of medical imaging.

The CLIP model contains two encoders: a Text encoder and an Image encoder. Whichever form of input it's given, it feeds it to the appropriate encoder.
CLIP is a model use by OpenAI to train Dall-E. CLIP generates 
1. CLIP generates the embeddings of the user's text prompt.
2. The images generated by Dall-E are also fed to CLIP to generate embedding for the image. 
3. The two embeddings are compared to determine whether they are in sync.
As such, CLIP was trained to learn & represent visual concepts from natural language supervision (i.e., the associated text caption/description). CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized.

## CLIP Achievements
1. It demonstrated that scaling a simple pre-training task is sufficient to achieve competitive zero-shot performance on a great variety of image classification datasets. CLIP models can be applied to nearly arbitrary visual classification tasks.
	- The pre-training task it scales is that of image-caption association. Previously, this contrastive approach was adopted by ConVIRT, but was limited to the field of medical imaging.
2. This approach helps OpenAI mitigate three critical weaknesses in the AI ecosystem:
	1. Leveraging contrastive learning, the model learn 1 embedding representation for an object's  textual and visual representation.
	2. **Narrowness:** For example, an ImageNet model is good at predicting the 1000 ImageNet categories, but that’s all it can do “out of the box.” If we wish to perform any other task, an ML practitioner needs to build a new dataset, add an output head, and fine-tune the model. In contrast, CLIP can be adapted to perform a wide variety of visual classification tasks without needing additional training examples. To apply CLIP to a new task, all we need to do is “tell” CLIP’s text-encoder the names of the task’s visual concepts, and it will output a linear classifier of CLIP’s visual representations. The accuracy of this classifier is often competitive with fully supervised models.
	3. This allows the model to generalize well over several benchmarks, which measure varied vision aspects. For instance,
		1. ObjectNet checks a model’s ability to recognize objects in many different poses and with many different backgrounds inside homes; while 
		2. ImageNet Rendition and ImageNet Sketch check a model’s ability to recognize more abstract depictions of objects.
3. Gathering the data to train CLIP proved to be much simpler, since they uses an abundantly available source of supervision: the text paired with images found across the internet. As such, they didn't need human labelers to create this dataset, and dataset preparation proved to be cheap. 

## CLIP Training approach
1. OpenAI scraped the internet for images-text pairs.
2. From this dataset, they would sample 1 image and 32,768 text snippets.
	- All of the classes in the dataset are converted into captions such as “a photo of a dog”.
3. CLIP was then given this proxy training task: given an image, predict which out of a set of 32,768 randomly sampled text snippets, was actually paired with it in our dataset.
	- The intuition behind this training task was that the CLIP models will need to learn to recognize a wide variety of visual concepts in images and associate them with their names.
	- 
> [!info] 
> OpenAI's article talks of CLIP in plural, as *"CLIP models"* because CLIP is composed of two encoder models: one image encoder model; other text encoder model.

