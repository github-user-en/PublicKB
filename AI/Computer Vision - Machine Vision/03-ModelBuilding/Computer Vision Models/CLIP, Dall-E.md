# Dall-E
Dall-E works in a 3-step process:
1. It encodes the user's text prompt;
2. It looks through its latent space to find an image's embedding similar to the user prompt embedding (similarity measured using Cosine Similarity, I guess);
3. Considering the found image embedding as the Prior, it generates probable images for the user.

# CLIP
The CLIP model contains two encoders: a Text encoder and an Image encoder. Whichever form of input it's given, it feeds it to the appropriate encoder.
CLIP is a model use by OpenAI to train Dall-E. CLIP generates 
The images generated by Dall-E are fed to CLIP. CLIP generates embeddings for those images and compares them with the embeddings of the user's text prompt.
