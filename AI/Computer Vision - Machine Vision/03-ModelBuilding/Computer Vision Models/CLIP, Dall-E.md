# Dall-E
Dall-E works in a 3-step process:
1. It encodes the user's text prompt;
2. It looks through its latent space to find an image's embedding similar to the user prompt embedding (similarity measured using Cosine Similarity, I guess);
3. Considering the found image embedding as the Prior, it generates probable images for the user.

# CLIP
> [!info] Resources
> - https://www.youtube.com/watch?v=8sVgLz3-W_s
> - https://www.youtube.com/watch?v=jwZQD0Cqz4o

The CLIP model contains two encoders: a Text encoder and an Image encoder. Whichever form of input it's given, it feeds it to the appropriate encoder.
CLIP is a model use by OpenAI to train Dall-E. CLIP generates 
1. CLIP generates the embeddings of the user's text prompt.
2. The images generated by Dall-E are also fed to CLIP to generate embedding for the image. 
3. The two embeddings are compared to determine whether they are in sync.
As such, CLIP was trained to learn & represent visual concepts from natural language supervision (i.e., the associated text caption/description). CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized.